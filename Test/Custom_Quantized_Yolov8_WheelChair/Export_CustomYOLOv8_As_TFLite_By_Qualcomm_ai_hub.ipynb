{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "collapsed": true,
        "id": "v4NFhvuA4pd-",
        "outputId": "0ef89d6c-3a06-4e08-d4e1-b361bdea2ae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.4.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "3188855a13144034a378bb66e0a15e6a",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.23.5  # 또는 1.24.3 정도로 낮춰 설치\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0YyEzn2pKmvd",
        "outputId": "b35f512c-985e-4f62-ca85-450fa821528f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q qai-hub onnx numpy==1.26.4\n",
        "\n",
        "import qai_hub as hub\n",
        "import onnx\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w0qXsj8bKr3R",
        "outputId": "4293630a-37a1-4f5c-ee99-2b21ed4e76c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qai-hub configuration saved to /root/.qai_hub/client.ini\n",
            "==================== /root/.qai_hub/client.ini ====================\n",
            "[api]\n",
            "api_token = 42f67116ca75d4c803448ee68598707d3861ab6a\n",
            "api_url = https://app.aihub.qualcomm.com\n",
            "web_url = https://app.aihub.qualcomm.com\n",
            "verbose = True\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ✅ 본인 API 토큰 입력\n",
        "!qai-hub configure --api_token 42f67116ca75d4c803448ee68598707d3861ab6a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iRySiXP1Kyfz",
        "outputId": "7c6461a1-417d-416a-e96f-ab0f4e26da6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading yolov8n.onnx\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 11.7M/11.7M [00:00<00:00, 36.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scheduled compile job (jgzoly8o5) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/jgzoly8o5/\n",
            "\n",
            "Waiting for compile job (jgzoly8o5) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ✅ SUCCESS                          \u0007\n",
            "✅ ONNX compile 완료\n"
          ]
        }
      ],
      "source": [
        "device = hub.Device(\"RB3 Gen 2 (Proxy)\")\n",
        "\n",
        "compile_job = hub.submit_compile_job(\n",
        "    model=\"yolov8n.onnx\",\n",
        "    device=device,\n",
        "    input_specs={\"images\": (1, 3, 640, 640)},\n",
        "    options=\"--target_runtime onnx\"\n",
        ")\n",
        "optimized_model = compile_job.get_target_model()\n",
        "print(\"✅ ONNX compile 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tTSWL4KK7fP",
        "outputId": "3d27fc06-02a7-44d7-b519-49dfbd450468"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Calibration 이미지 10장 로드 완료\n"
          ]
        }
      ],
      "source": [
        "def load_calibration_images(folder_path, input_shape=(1, 3, 640, 640)):\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape((1, 3, 1, 1))  # NCHW\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape((1, 3, 1, 1))\n",
        "    images = []\n",
        "\n",
        "    for fname in sorted(os.listdir(folder_path))[:20]:\n",
        "        path = os.path.join(folder_path, fname)\n",
        "        img = Image.open(path).convert(\"RGB\").resize((640, 640))\n",
        "        img_np = np.array(img).astype(np.float32) / 255.0  # HWC, 0~1\n",
        "        img_np = np.transpose(img_np, (2, 0, 1))  # CHW\n",
        "        img_np = np.expand_dims(img_np, axis=0)  # NCHW\n",
        "        img_np = (img_np - mean) / std\n",
        "        images.append(img_np.astype(np.float32))\n",
        "\n",
        "    return images\n",
        "\n",
        "calibration_data = {\"images\": load_calibration_images(\"calib_images\")}\n",
        "print(f\"✅ Calibration 이미지 {len(calibration_data['images'])}장 로드 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScOehE74LzJx",
        "outputId": "cc98cf77-7ad0-4a55-af75-facb40275d9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Uploading dataset: 12.5MB [00:00, 37.4MB/s]                            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scheduled quantize job (j5mq0kodp) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/j5mq0kodp/\n",
            "\n",
            "Waiting for quantize job (j5mq0kodp) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ✅ SUCCESS                          \u0007\n",
            "✅ 양자화 완료\n"
          ]
        }
      ],
      "source": [
        "quant_job = hub.submit_quantize_job(\n",
        "    model=optimized_model,  # 앞에서 compile 완료된 ONNX 모델\n",
        "    calibration_data=calibration_data,  # 지금 만든 실사 이미지 기반\n",
        "    weights_dtype=hub.QuantizeDtype.INT8,\n",
        "    activations_dtype=hub.QuantizeDtype.INT8,\n",
        ")\n",
        "\n",
        "quantized_model = quant_job.get_target_model()\n",
        "print(\"✅ 양자화 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PiIEA2JMFsg",
        "outputId": "76d7cbf0-e62f-41cd-926d-7cf30281da00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scheduled compile job (jp20rd4rg) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/jp20rd4rg/\n",
            "\n",
            "Waiting for compile job (jp20rd4rg) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ✅ SUCCESS                          \u0007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wheelChair_YOLOv8-Detection-Quantized.tflite: 100%|\u001b[34m██████████\u001b[0m| 3.10M/3.10M [00:00<00:00, 44.0MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded model to wheelChair_YOLOv8-Detection-Quantized.tflite\n",
            "✅ 최종 TFLite 모델 다운로드 완료\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tflite_job = hub.submit_compile_job(\n",
        "    model=quantized_model,\n",
        "    device=device,\n",
        "    options=\"--target_runtime tflite --quantize_io\"\n",
        ")\n",
        "\n",
        "tflite_model = tflite_job.get_target_model()\n",
        "tflite_model.download(\"wheelChair_YOLOv8-Detection-Quantized.tflite\")\n",
        "print(\"✅ 최종 TFLite 모델 다운로드 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oOPE3P3aMaAH"
      },
      "outputs": [],
      "source": [
        "# 예시: test_image.jpg를 추론용 input으로 준비\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "image_path = \"calib_images/pexels-elevate-3009792.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\").resize((640, 640))\n",
        "img_np = np.array(image).astype(np.float32) / 255.0\n",
        "img_np = np.transpose(img_np, (2, 0, 1))  # CHW\n",
        "img_np = np.expand_dims(img_np, axis=0)  # NCHW\n",
        "\n",
        "input_tensor = img_np.astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NjiWMSsMiY6",
        "outputId": "388cfc84-444c-493e-900e-fe1c0837711f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Uploading dataset: 1.54MB [00:00, 8.39MB/s]                   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scheduled inference job (jpee7xlop) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/jpee7xlop/\n",
            "\n",
            "Waiting for inference job (jpee7xlop) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ✅ SUCCESS                          \u0007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tmp6c0200tj.h5: 100%|\u001b[34m██████████\u001b[0m| 14.4k/14.4k [00:00<00:00, 12.5MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 추론 완료, 출력 키: ['output_0']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "inference_job = hub.submit_inference_job(\n",
        "    model=tflite_model,  # 또는 quantized_model도 가능\n",
        "    device=device,       # RB3 Gen 2 또는 Galaxy S24 등\n",
        "    inputs={\"images\": [input_tensor]}\n",
        ")\n",
        "\n",
        "# 결과 다운로드\n",
        "output = inference_job.download_output_data()\n",
        "print(\"✅ 추론 완료, 출력 키:\", list(output.keys()))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
