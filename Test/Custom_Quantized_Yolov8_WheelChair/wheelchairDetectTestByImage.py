import cv2
import numpy as np
from tflite_runtime.interpreter import Interpreter

# ========================
# 1. Quantized TFLite ëª¨ë¸ ë¡œë“œ
# ========================
interpreter = Interpreter(model_path="wheelChair_YOLOv8-Detection-Quantized.tflite")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print("ğŸ“¥ ì…ë ¥ í…ì„œ ì •ë³´:", input_details[0])
print("ğŸ“¤ ì¶œë ¥ í…ì„œ ì •ë³´:", output_details[0])

# ========================
# 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
# - ì´ë¯¸ì§€ ë¡œë“œ ë° ë¦¬ì‚¬ì´ì§•
# - RGB ë³€í™˜, ì •ê·œí™” í›„ ì–‘ìí™” ì ìš©
# - ëª¨ë¸ ì…ë ¥ í…ì„œë¡œ ë³€í™˜
# ========================
def preprocess_image(image_path, input_shape=(640, 640)):
    image = cv2.imread(image_path)
    if image is None:
        raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    print(f"ğŸ–¼ï¸ ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")

    image_resized = cv2.resize(image, input_shape)
    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)
    image_normalized = image_rgb.astype(np.float32) / 255.0

    scale, zero_point = input_details[0]['quantization']
    print(f"ğŸ§® ì…ë ¥ quantization scale={scale}, zero_point={zero_point}")
    image_quantized = image_normalized / scale + zero_point
    image_quantized = np.clip(image_quantized, -128, 127).astype(np.int8)

    print("ğŸ” ì…ë ¥ í…ì„œ min/max:", image_quantized.min(), image_quantized.max())
    input_tensor = np.expand_dims(np.transpose(image_quantized, (2, 0, 1)), axis=0)
    return image, input_tensor

# ========================
# 3. IoU (Intersection over Union) ê³„ì‚° í•¨ìˆ˜
# - ë‘ ë°”ìš´ë”© ë°•ìŠ¤ì˜ ê²¹ì¹˜ëŠ” ì •ë„ ê³„ì‚°
# ========================
def iou(box1, box2):
    x1, y1, x2, y2 = box1
    x1_p, y1_p, x2_p, y2_p = box2

    inter_x1 = max(x1, x1_p)
    inter_y1 = max(y1, y1_p)
    inter_x2 = min(x2, x2_p)
    inter_y2 = min(y2, y2_p)

    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
    box1_area = (x2 - x1) * (y2 - y1)
    box2_area = (x2_p - x1_p) * (y2_p - y1_p)
    union = box1_area + box2_area - inter_area

    return inter_area / union if union != 0 else 0

# ========================
# 4. NMS (Non-Max Suppression) ì ìš© í•¨ìˆ˜
# - ë†’ì€ confidence ê°’ì„ ê°€ì§„ ë°•ìŠ¤ë§Œ ë‚¨ê¸°ê³  ì¤‘ë³µ ì œê±°
# ========================
def nms(boxes, iou_thresh=0.5):
    boxes = sorted(boxes, key=lambda x: x[4], reverse=True)
    selected = []

    while boxes:
        current = boxes.pop(0)
        selected.append(current)
        boxes = [b for b in boxes if iou(current[:4], b[:4]) < iou_thresh]

    return selected

# ========================
# 5. í›„ì²˜ë¦¬ í•¨ìˆ˜
# - ì¶œë ¥ í…ì„œ ë””í€€íƒ€ì´ì¦ˆ
# - confidence ê¸°ë°˜ í•„í„°ë§
# - bounding box ê³„ì‚°
# - NMS ì ìš© í›„ ìµœì¢… ë°•ìŠ¤ ì‹œê°í™”
# ========================
def postprocess(output_data, image, conf_threshold=0.4):
    output = output_data[0]  # (1, 5, 8400)
    print("ğŸ“¤ ì¶œë ¥ shape:", output.shape)
    scale, zero_point = output_details[0]['quantization']
    print(f"ğŸ“ ì¶œë ¥ quantization scale={scale}, zero_point={zero_point}")

    output = (output.astype(np.float32) - zero_point) * scale
    output = np.transpose(output[0])  # (8400, 5)

    conf_values = output[:, 4]
    print(f"ğŸ“Š confidence ê°’ í†µê³„: min={conf_values.min():.4f}, max={conf_values.max():.4f}, avg={conf_values.mean():.4f}")
    print("ğŸ“‹ ìƒìœ„ 10ê°œ confidence:", np.sort(conf_values)[-10:])

    h, w, _ = image.shape
    boxes = []

    for det in output:
        x_c, y_c, box_w, box_h, conf = det
        if conf < conf_threshold:
            continue

        x1 = int((x_c - box_w / 2) * w)
        y1 = int((y_c - box_h / 2) * h)
        x2 = int((x_c + box_w / 2) * w)
        y2 = int((y_c + box_h / 2) * h)
        boxes.append((x1, y1, x2, y2, conf))

    if not boxes:
        print("âŒ íƒì§€ ê²°ê³¼ ì—†ìŒ")
        return image

    # NMS ì ìš©
    boxes = nms(boxes, iou_thresh=0.5)
    print(f"âœ… NMS ì´í›„ ìµœì¢… ë°•ìŠ¤ ê°œìˆ˜: {len(boxes)}")

    for x1, y1, x2, y2, conf in boxes:
        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        label = f"people_wheelchair {conf:.2f}"
        cv2.putText(image, label, (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        print(f"ğŸŸ¢ íƒì§€ë¨: people_wheelchair (ì‹ ë¢°ë„: {conf:.2f})")

    return image

# ========================
# 6. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# - ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° â†’ ì „ì²˜ë¦¬ â†’ ì¶”ë¡  â†’ í›„ì²˜ë¦¬
# ========================
if __name__ == "__main__":
    image_path = "calib_images/1.jpg"
    orig_image, input_tensor = preprocess_image(image_path)

    interpreter.set_tensor(input_details[0]['index'], input_tensor)
    interpreter.invoke()
    output_data = [interpreter.get_tensor(out['index']) for out in output_details]

    output_image = postprocess(output_data, orig_image)
    cv2.imwrite("images/Output/output_tflite_detected.jpg", output_image)
    print("âœ… ê²°ê³¼ ì €ì¥ë¨: images/Output/output_tflite_detected.jpg")
